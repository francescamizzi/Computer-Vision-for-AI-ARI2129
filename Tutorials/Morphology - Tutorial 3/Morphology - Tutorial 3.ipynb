{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology - Tutorial 3\n",
    "\n",
    "ARI2129 - Principles of Computer Vision for AI\n",
    "\n",
    "Francesca Maria Mizzi - 118201L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin manipulating the desired images, the required libraries must first be imported. In the case of this tutorial I made use of only 2 libraries:\n",
    " - OpenCv (cv2) for image processing\n",
    " - numpy for it's arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then read the required images and saved them in variables. \n",
    "\n",
    "I imported the coins photo and the shapes photo immediatley as grayscale image since the function for connected components requires them to be in grayscale. I chose however to keep the text photo in BGR since it doesn't really make a difference for the remaining functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = cv2.imread('Photos/Euro_Coins.jpg',0)\n",
    "shapes = cv2.imread('Photos/shapes.jpg',0)\n",
    "text = cv2.imread('Photos/text.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to segment and labels all the different items in the photos, I carried out the following steps:\n",
    " - Added a gaussian blur to the photo in order to reduce noise\n",
    " - Thresholded the image\n",
    " - Applied the connectedComponenets() method\n",
    " \n",
    " I then added colour labels to the photo in order to show the different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(raw):\n",
    "    blur = cv2.GaussianBlur(raw,(13,13),cv2.BORDER_DEFAULT)\n",
    "    ret, img = cv2.threshold(blur,230,255,cv2.THRESH_BINARY_INV)\n",
    "    num_labels, labels = cv2.connectedComponents(img, connectivity=4)\n",
    "    \n",
    "    label_hue = np.uint8(179 * labels / np.max(labels))\n",
    "    blank_ch = 255 * np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "    labeled_img[label_hue == 0] = 0\n",
    "\n",
    "    cv2.imshow('Segments', labeled_img)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_components(shapes)\n",
    "get_components(coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the connectivity at both 4 and 8, I didn't really see a difference between the photos but after some research, the reason for this is that all the shapes and coins are so distinctly seperated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dilation, I created a function which takes the image as well as the strength of the dilation and performs dilation on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(img,kernel):\n",
    "    kernel = np.ones((kernel, kernel), 'uint8')\n",
    "    dilation = cv2.dilate(img.copy(),kernel)\n",
    "    \n",
    "    cv2.imshow('Dilation', dilation)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilate(text,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing the function on a variety of kernels, I found that the larger the kernel, the lighter the text on the image becomes with, in the case of the text image used, a 4x4 kernel seems to be the strongest to where the text is still legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Erosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My erosion function is very similar to the dilation function except for the different OpenCV method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erosion(img,kernel):\n",
    "    kernel = np.ones((kernel, kernel), 'uint8')\n",
    "    erosion = cv2.erode(img.copy(),kernel)\n",
    "    \n",
    "    cv2.imshow('Erosion', erosion)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion(text,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing the function, I found that the larger the kernel, the bolder the text in the photo becomes with, in the case of the text image used, a 6x6 kernel beeing the strongest to where the text is still legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Opening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function to perform opening on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opening(img,kernel):\n",
    "    kernel = np.ones((kernel, kernel), 'uint8')\n",
    "    opening = cv2.morphologyEx(img.copy(), cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    cv2.imshow('Opening', opening)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening(text,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, I found that the larger the kernel, the more smudged the image appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created another function to perform closing on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closing(img,kernel):\n",
    "    kernel = np.ones((kernel, kernel), 'uint8')\n",
    "    closing = cv2.morphologyEx(img.copy(), cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    cv2.imshow('Closing', closing)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing(text,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, I found that the larger the kernel, the more blurred the image looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around with a few options in order to segment the text image, I didn't manage to successfully complete the task. However, the below method was the closest I could make to the request.\n",
    "\n",
    "I turned the image to grayscale and thresholded it and then found the histogram for that image. I then found upper and lower bounds based off of the h istogram and converted the image back to colour and tried to draw all those bounds but it did not draw as I wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(raw):\n",
    "    raw = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(raw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    hist = cv2.reduce(thresh,1, cv2.REDUCE_AVG).reshape(-1)\n",
    "    \n",
    "    th = 2\n",
    "    H,W = raw.shape[:2]\n",
    "    uppers = [y for y in range(H-1) if hist[y]<=th and hist[y+1]>th]\n",
    "    lowers = [y for y in range(H-1) if hist[y]>th and hist[y+1]<=th]\n",
    "\n",
    "    thresh = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for y in uppers:\n",
    "        cv2.line(thresh, (0,y), (W, y), (255,0,0), 1)\n",
    "\n",
    "    for y in lowers:\n",
    "        cv2.line(thresh, (0,y), (W, y), (0,255,0), 1)\n",
    "\n",
    "    cv2.imshow(\"result.png\", thresh)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
